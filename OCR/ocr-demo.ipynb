{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Digitize your notes with Azure Computer Vision OCR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Computer Vision service provides pre-built, advanced algorithms that process and analyze images and extract text from photos and documents (Optical Character Recognition, OCR).\r\n",
    "\r\n",
    "In this exercise, we will explore the pre-trained models of Azure Computer Vision service for optical character recognition. We will build a simple Python notebook that turns your handwritten documents into digital notes. \r\n",
    "\r\n",
    "You will learn how to:\r\n",
    "* Provision a Computer Vision resource.\r\n",
    "* Use a Computer Vision resource to extract text from photos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Computer Vision Resource"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Sign in to [Azure Portal](https://portal.azure.com/) and select **Create a resource**.\r\n",
    "2. Search for **Computer Vision** and then click **Create**.\r\n",
    "3. Create a Computer Vision resource with the following settings:\r\n",
    "    * **Subscription**: Your Azure subscription.\r\n",
    "    * **Resource group**: Select an existing resource group or create a new one.\r\n",
    "    * **Region**: Choose any available region, for example **North Europe**.\r\n",
    "    * **Name**: This would be your custom domain name in your endpoint. Enter a unique name.\r\n",
    "    * **Pricing tier**: You can use the free pricing tier (**F0**) to try the service, and upgrade later to a paid tier.\r\n",
    "4. Select **Review + Create** and wait for deployment to complete.\r\n",
    "5. One the deployment is complete, select **Go to resource**. On the **Overview** tab, click **Manage keys**. Save the **Key 1** and the **Endpoint**. You will need the key and the endpoint to connect to your Computer Vision resource from client applications.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\n",
    "from msrest.authentication import CognitiveServicesCredentials\r\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\n",
    "from PIL import Image\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.patches as patches\r\n",
    "import time\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create variables for your key and endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "key = 'YOUR_KEY'\r\n",
    "endpoint = 'YOUR_ENDPOINT'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Authenticate the client"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(key))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract handwritten text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First download the images used in the following examples from my [GitHub repository](https://github.com/sfoteini/azure-computer-vision)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_handwritten_text(image_name):\r\n",
    "    # Open local image file\r\n",
    "    image_path = \"images/\" + image_name\r\n",
    "    image = open(image_path, \"rb\")\r\n",
    "\r\n",
    "    img = Image.open(image_path)\r\n",
    "\r\n",
    "    # Call the API\r\n",
    "    read_response = computervision_client.read_in_stream(image, raw=True)\r\n",
    "\r\n",
    "    # Get the operation location (URL with an ID at the end)\r\n",
    "    read_operation_location = read_response.headers[\"Operation-Location\"]\r\n",
    "\r\n",
    "    # Grab the ID from the URL\r\n",
    "    operation_id = read_operation_location.split(\"/\")[-1]\r\n",
    "\r\n",
    "    # Retrieve the results \r\n",
    "    while True:\r\n",
    "        read_result = computervision_client.get_read_result(operation_id)\r\n",
    "        if read_result.status not in ['notStarted', 'running']:\r\n",
    "            break\r\n",
    "        time.sleep(1)\r\n",
    "\r\n",
    "    # Create figure and axes\r\n",
    "    fig, ax = plt.subplots()\r\n",
    "\r\n",
    "    # Display the image\r\n",
    "    ax.imshow(img)\r\n",
    "\r\n",
    "    # Print the detected text and bounding boxes\r\n",
    "    if read_result.status == OperationStatusCodes.succeeded:\r\n",
    "        for text_result in read_result.analyze_result.read_results:\r\n",
    "            for line in text_result.lines:\r\n",
    "                # Print line\r\n",
    "                print(line.text)\r\n",
    "\r\n",
    "                # line.bounding_box contains 4 pairs of (x, y) coordinates\r\n",
    "                xy1 = [line.bounding_box[0], line.bounding_box[1]]\r\n",
    "                xy2 = [line.bounding_box[2], line.bounding_box[3]]\r\n",
    "                xy3 = [line.bounding_box[4], line.bounding_box[5]]\r\n",
    "                xy4 = [line.bounding_box[6], line.bounding_box[7]]\r\n",
    "                box_coordinates = np.array([xy1, xy2, xy3, xy4])\r\n",
    "                \r\n",
    "                # Create a Rectangle patch\r\n",
    "                box = patches.Polygon(box_coordinates, closed=True, linewidth=2, edgecolor='r', facecolor='none')\r\n",
    "\r\n",
    "                # Add the patch to the Axes\r\n",
    "                ax.add_patch(box)\r\n",
    "\r\n",
    "                # Print words in line with confidence score\r\n",
    "                for word in line.words:\r\n",
    "                    print(f\"   * {word.text}: {word.confidence * 100:.2f}\")\r\n",
    "                    \r\n",
    "                    # Uncomment the following lines to display the bounding box of each word\r\n",
    "                    '''\r\n",
    "                    xy1 = [word.bounding_box[0], word.bounding_box[1]]\r\n",
    "                    xy2 = [word.bounding_box[2], word.bounding_box[3]]\r\n",
    "                    xy3 = [word.bounding_box[4], word.bounding_box[5]]\r\n",
    "                    xy4 = [word.bounding_box[6], word.bounding_box[7]]\r\n",
    "                    box_coordinates = np.array([xy1, xy2, xy3, xy4])\r\n",
    "                \r\n",
    "                    # Create a Rectangle patch\r\n",
    "                    box = patches.Polygon(box_coordinates, closed=True, linewidth=1, edgecolor='c', facecolor='none')\r\n",
    "\r\n",
    "                    # Add the patch to the Axes\r\n",
    "                    ax.add_patch(box)\r\n",
    "                    '''\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_handwritten_text(\"notes1.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_handwritten_text(\"notes2.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_handwritten_text(\"notes3.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_handwritten_text(\"notes4.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "2deb099b60a8b4da913787c955f1d57026f67c6109413fa49af9a1fc936f94ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}